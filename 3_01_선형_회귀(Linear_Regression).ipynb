{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3-01. 선형 회귀(Linear Regression)",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOLQCZBjToatEb/1qL/nzC/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yangyeeun/20MachineLearning/blob/master/3_01_%EC%84%A0%ED%98%95_%ED%9A%8C%EA%B7%80(Linear_Regression).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpTPKGL3e7fY",
        "colab_type": "text"
      },
      "source": [
        "1.   데이터에 대한 이해(Data Definition)\n",
        "2.   가설(Hypothesis) 수립\n",
        "3.   손실 계산하기(Compute loss)\n",
        "4.   경사 하강법(Gradient Descent)\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "#1. 데이터에 대한 이해(Data Definition)\n",
        "\n",
        "\n",
        "\n",
        "##** 1) 훈련 데이터셋과 테스트 데이터셋**\n",
        "공부한 시간과 점수에 대한 상관관계 \n",
        "\n",
        " ![대체 텍스트](https://wikidocs.net/images/page/53560/data_definition.PNG)\n",
        "\n",
        "훈련 데이터셋(training dataset) 예측\n",
        "테스트 데이터셋(test dataset) 학습이 끝난 후, 이 모델이 얼마나 잘 작동하는지 판별\n",
        "\n",
        "## **2) 훈련 데이터셋의 구성**\n",
        "모델을 학습시키기 위한 데이터  \n",
        "->텐서의 형태(torch.tensor) & 입력은 x, 출력은 y"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYyY5lKheZls",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#x_train(입력):공부한 시간, y_train(출력): 그에 맵핑되는 점수\n",
        "x_train = torch.FloatTensor([[1], [2], [3]])\n",
        "y_train = torch.FloatTensor([[2], [4], [6]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8V9qJm4e7Ce",
        "colab_type": "text"
      },
      "source": [
        "# 2. 가설(Hypothesis) 수립\n",
        "\n",
        "**가설**: 머신 러닝에서 식을 세울때의 식  \n",
        "-> 임의로 추측해서 세워볼 수도 있고, 경험적으로 알고 있을 수도  \n",
        "-> 맞는 가설이 아니라고 판단되면 계속 수정해나가게\n",
        "\n",
        "**선형 회귀**: 학습데이터와 제일 잘맞는 하나의 직선을 탐색  \n",
        "\n",
        "선형 회귀의 가설: ***H(x)=Wx+b*** -> W = 가중치(Weight), b = 편향(bias)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# 3. 비용 함수(Cost function)에 대한 이해\n",
        "\n",
        "비용 cost = 손실 loss = 오차 error = 목적 object 함수\n",
        "\n",
        "4개 훈련데이터 -> 2차원 그래프에 표현 -> 4개점을 가장 잘 표현하는 직선을 찾는 것이 목표!\n",
        "\n",
        "\n",
        "![대체 텍스트](https://wikidocs.net/images/page/53560/%EA%B7%B8%EB%A6%BC2.PNG)\n",
        "![대체 텍스트](https://wikidocs.net/images/page/53560/%EA%B7%B8%EB%A6%BC3.PNG)  \n",
        "\n",
        "오차↕ = 직선의 예측값과 실제값(4개의 점)의 차이   \n",
        "∑[y(i)−H(x(i))]^2 = (−2)2+102+(−7)2+(−5)2 = 178  \n",
        "\n",
        "\n",
        "\n",
        " '오차 = 실제값 - 예측값' => +,- 값 모두 나옴  \n",
        "\n",
        "\n",
        " 평균 제곱 오차(Mean Squared Error, MSE): 오차의 제곱합의 평균  \n",
        " -> Cost(W,b)가 최소가 되는 W,b를 구하자\n",
        "\n",
        "```\n",
        "# cost(W,b)=1/n∑[y(i)−H(x(i))]^2\n",
        "```\n",
        "\n",
        " 1/n∑[y(i)−H(x(i))]^2 = 178/4 = 44.5\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "# 4. 옵티마이저 - 경사 하강법(Gradient Descent)\n",
        "\n",
        "**옵티마이저** =최적화 알고리즘  中 경사하강법  \n",
        "\n",
        "**학습**: 옵티마이저 알고리즘을 통해 적절한 W,b를 찾는 과정  \n",
        "\n",
        "![대체 텍스트](https://wikidocs.net/images/page/53560/%EA%B7%B8%EB%A6%BC4.PNG)  \n",
        "주황색선-> 기울기 W= 20 => y=20x \n",
        "초록색선-> 기울기 W= 1 => y=x \n",
        "\n",
        "기울기W &b가 지나치게 작거나 크면 실제값과 예측값의 오차↕가 커집니다. \n",
        "\n",
        "\n",
        "\n",
        " W와 cost의 관계 -> **경사하강법**\n",
        "\n",
        "\n",
        "기울기W &b가 지나치게 작거나 크면 실제값과 예측값의 오차↕가 무한대로 발산  \n",
        " ![대체 텍스트](https://wikidocs.net/images/page/21670/%EA%B8%B0%EC%9A%B8%EA%B8%B0%EC%99%80%EC%BD%94%EC%8A%A4%ED%8A%B8.PNG)    \n",
        "\n",
        "  \n",
        " cost가 가장 최소값인 W를 찾아야 -> 접선의 기울기=0   \n",
        " ![대체 텍스트](https://wikidocs.net/images/page/21670/%EA%B2%BD%EC%82%AC%ED%95%98%EA%B0%95%EB%B2%95.PNG)\n",
        "\n",
        "임의의 초기값에서 점차 수정해 맨아래 볼록한 부분인 접선의 기울기가 0 이 되는 W를 찾기\n",
        "\n",
        " ![대체 텍스트](https://wikidocs.net/images/page/21670/%EC%A0%91%EC%84%A0%EC%9D%98%EA%B8%B0%EC%9A%B8%EA%B8%B01.PNG)\n",
        "\n",
        "\n",
        "기울기=∂cost(W)/∂W\n",
        "\n",
        "`W:=W−α*∂/∂W*cost(W)`\n",
        "\n",
        "\n",
        "**α학습률**\n",
        "=  W의 값을 변경할 때, 얼마나 크게 변경할지를 결정\n",
        "=  그래프에서 접선의 기울기가 0까지 경사를 타고 내려간다는 관점에서 얼마나 큰폭으로 이동할지 결정  \n",
        "\n",
        "\n",
        "![대체 텍스트](https://wikidocs.net/images/page/21670/%EA%B8%B0%EC%9A%B8%EA%B8%B0%EB%B0%9C%EC%82%B0.PNG)\n",
        "\n",
        "=> 학습률이 지나치게 크면 W값이 발산하고  \n",
        "지나치게 작으면 학습속도가 느려지므로   \n",
        "적당한 값을 찾는 것이 중요!\n",
        "\n",
        "\n",
        "cf) 가설, 비용 함수, 옵티마이저는 문제마다 다 다르게 설정 가능하지만\n",
        "선형회귀에는 비용 함수는 평균 제곱 오차, 옵티마이저는 경사 하강법이 가장 적합  \n",
        "\n",
        "---\n",
        "#4. 파이토치로 선형 회귀 구현하기\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siZbe9WuX2Aj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1) 기본 셋팅\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# 랜덤 시드(random seed): 현재 실습하고 있는 파이썬 코드를 재실행해도 다음에도 같은 결과가 나옴.\n",
        "torch.manual_seed(1)\n",
        "\n",
        "# 2) 변수선언 -> x_train(입력):공부한 시간, y_train(출력): 그에 맵핑되는 점수\n",
        "x_train = torch.FloatTensor([[1], [2], [3]])\n",
        "y_train = torch.FloatTensor([[2], [4], [6]])\n",
        "\n",
        "# 3) 가중치와 편향의 초기화\n",
        "# 가중치 W를 0으로 초기화, requires_grad=  학습을 통해 값이 변경되는 변수임\n",
        "W = torch.zeros(1, requires_grad=True)\n",
        "b = torch.zeros(1, requires_grad=True)\n",
        "\n",
        "# 6) 경사 하강법 구현하기\n",
        "#SGD= 경사 하강법 lr= 학습률(learning rate)\n",
        "optimizer = optim.SGD([W, b], lr=0.01)\n",
        "\n",
        "nb_epochs = 2000 # 원하는만큼 경사 하강법을 반복\n",
        "for epoch in range(nb_epochs + 1):\n",
        "\n",
        "    # 4)가설 세우기\n",
        "    hypothesis = x_train * W + b\n",
        "\n",
        "    # 5)비용함수 선언하기\n",
        "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
        "\n",
        "    # 6) 경사 하강법 구현하기\n",
        "    # gradient를 0으로 초기화\n",
        "    optimizer.zero_grad() \n",
        "    # 비용 함수를 미분하여 gradient 계산\n",
        "    cost.backward() \n",
        "    # W와 b를 업데이트=기울기에 학습률 0.01을 곱하여 빼줌\n",
        "    optimizer.step()\n",
        "\n",
        "    # 100번마다 로그 출력\n",
        "    if epoch % 100 == 0:\n",
        "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
        "            epoch, nb_epochs, W.item(), b.item(), cost.item()\n",
        "        ))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}