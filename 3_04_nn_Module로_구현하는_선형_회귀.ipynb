{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3-04. nn.Module로 구현하는 선형 회귀",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPk7NNr+MzvxHi4hKIdUDO8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yangyeeun/20MachineLearning/blob/master/3_04_nn_Module%EB%A1%9C_%EA%B5%AC%ED%98%84%ED%95%98%EB%8A%94_%EC%84%A0%ED%98%95_%ED%9A%8C%EA%B7%80.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwoH6YWuykgn",
        "colab_type": "text"
      },
      "source": [
        "# 04. nn.Module로 구현하는 선형 회귀\n",
        "\n",
        "---\n",
        "\n",
        "파이토치에서 이미 구현되어져 제공되고 있는 함수 호출하여 선형회귀 모델 구현해보자!   \n",
        "\n",
        "선형 회귀 모델:nn.Linear()  \n",
        " 평균 제곱오차:nn.functional.mse_loss()\n",
        "\n",
        "\n",
        " ## 4-1) 단순 선형 회귀 구현하기\n",
        "\n",
        "\n",
        " **forward 연산**\n",
        "*   H(x)  식에 입력 x로부터 예측된 y를 얻는 것\n",
        "*   학습 전, prediction = model(x_train)은 x_train으로부터 예측값을 리턴  \n",
        "*   학습 후, pred_y = model(new_var)는 임의의 값 new_var로부터 예측값을 리턴\n",
        "\n",
        "**backward 연산**\n",
        "*   학습 과정에서 비용 함수를 미분하여 기울기를 구하는 것\n",
        "*   cost.backward(): 비용 함수로부터 기울기를 구하라"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cycP19N8zZFj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "torch.manual_seed(1)\n",
        "\n",
        "# 데이터  W=2, b=0 예측\n",
        "x_train = torch.FloatTensor([[1], [2], [3]])\n",
        "y_train = torch.FloatTensor([[2], [4], [6]])\n",
        "\n",
        "# 모델을 선언 및 초기화. 단순 선형 회귀이므로 input_dim=1, output_dim=1.\n",
        "model = nn.Linear(1,1)\n",
        "\n",
        "# optimizer 설정. model.parameters(): W와 b를 전달, 경사 하강법 SGD를 사용하고 learning rate를 의미하는 lr은 0.01\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01) \n",
        "\n",
        "# 전체 훈련 데이터에 대해 경사 하강법을 2,000회 반복\n",
        "nb_epochs = 2000\n",
        "for epoch in range(nb_epochs+1):\n",
        "\n",
        "    # H(x) 계산\n",
        "    prediction = model(x_train)\n",
        "\n",
        "    # cost 계산\n",
        "    cost = F.mse_loss(prediction, y_train) # <== 파이토치에서 제공하는 평균 제곱 오차 함수\n",
        "\n",
        "    # cost로 H(x) 개선하는 부분\n",
        "    # gradient를 0으로 초기화\n",
        "    optimizer.zero_grad()\n",
        "    # 비용 함수를 미분하여 gradient 계산\n",
        "    cost.backward() # backward 연산\n",
        "    # W와 b를 업데이트\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "    # 100번마다 로그 출력\n",
        "      print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
        "          epoch, nb_epochs, cost.item()\n",
        "      ))\n",
        "\n",
        "# 임의의 입력 4를 선언\n",
        "new_var =  torch.FloatTensor([[4.0]]) \n",
        "# 입력한 값 4에 대해서 예측값 y를 리턴받아서 pred_y에 저장\n",
        "pred_y = model(new_var) # forward 연산\n",
        "# y = 2x 이므로 입력이 4라면 y가 8에 가까운 값이 나와야 제대로 학습이 된 것\n",
        "print(\"훈련 후 입력이 4일 때의 예측값 :\", pred_y)   #tensor([[7.9989]], grad_fn=<AddmmBackward>)\n",
        "\n",
        "#학습 후의 W와 b의 값을 출력\n",
        "print(list(model.parameters())) #W의 값이 2에 가깝고, b의 값이 0에 가까움"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gLS3kYTzYo8",
        "colab_type": "text"
      },
      "source": [
        "## 2. 다중 선형 회귀 구현하기\n",
        "\n",
        "\n",
        " nn.Linear()의 인자값& 학습률(learning rate)만 조절"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9X3o2dz6TeT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "torch.manual_seed(1)\n",
        "\n",
        "# 데이터\n",
        "x_train = torch.FloatTensor([[73, 80, 75],\n",
        "                             [93, 88, 93],\n",
        "                             [89, 91, 90],\n",
        "                             [96, 98, 100],\n",
        "                             [73, 66, 70]])\n",
        "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
        "\n",
        "# 모델을 선언 및 초기화. 다중 선형 회귀이므로 input_dim=3, output_dim=1.\n",
        "model = nn.Linear(3,1)\n",
        "\n",
        "# optimizer 설정. model.parameters(): W와 b를 전달, 경사 하강법 SGD를 사용하고 learning rate를 의미하는 lr은 0.01\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5) \n",
        "\n",
        "# 전체 훈련 데이터에 대해 경사 하강법을 2,000회 반복\n",
        "nb_epochs = 2000\n",
        "for epoch in range(nb_epochs+1):\n",
        "\n",
        "    # H(x) 계산\n",
        "    prediction = model(x_train)\n",
        "\n",
        "    # cost 계산\n",
        "    cost = F.mse_loss(prediction, y_train) # <== 파이토치에서 제공하는 평균 제곱 오차 함수\n",
        "\n",
        "    # cost로 H(x) 개선하는 부분\n",
        "    # gradient를 0으로 초기화\n",
        "    optimizer.zero_grad()\n",
        "    # 비용 함수를 미분하여 gradient 계산\n",
        "    cost.backward() # backward 연산\n",
        "    # W와 b를 업데이트\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "    # 100번마다 로그 출력\n",
        "      print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
        "          epoch, nb_epochs, cost.item()\n",
        "      ))\n",
        "\n",
        "# 임의의 입력 [73, 80, 75]를 선언\n",
        "new_var =  torch.FloatTensor([[73, 80, 75]]) \n",
        "# 입력한 값 [73, 80, 75]에 대해서 예측값 y를 리턴받아서 pred_y에 저장  y는 [152]\n",
        "pred_y = model(new_var) \n",
        "print(\"훈련 후 입력이 73, 80, 75일 때의 예측값 :\", pred_y) # tensor([[151.2305]], grad_fn=<AddmmBackward>)\n",
        "\n",
        "#학습 후의 W와 b의 값을 출력\n",
        "print(list(model.parameters()))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}